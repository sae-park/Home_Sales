# Home_Sales

Overview
This project involved analyzing a dataset of home sales using Apache Spark to practice distributed computing techniques. The goal was to leverage Spark's capabilities for handling large datasets efficiently, particularly using caching and partitioning strategies to compare performance impacts.

Objectives
Set up and configure a Spark environment in a Jupyter Notebook.

Read and load a real estate dataset from an AWS S3 bucket into a Spark DataFrame.

Perform data analysis and aggregations using Spark SQL and DataFrame operations.

Evaluate the performance of various Spark optimization techniques, including:

Caching the DataFrame.

Partitioning the DataFrame by specific columns (e.g., date_sold).

Measure and compare execution times of queries under different optimization strategies.
